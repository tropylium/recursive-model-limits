{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991bd0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Change to project root\n",
    "os.chdir(\"..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134ff411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from src.train_state import TrainState\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ============ CONFIGURATION ============\n",
    "# Change this to your run directory\n",
    "RUN_DIRECTORY = \"output/runs/simple_cnn@SimpleCNN_CIFAR10_20251201_214440\"\n",
    "# ======================================\n",
    "\n",
    "print(f\"Loading model from: {RUN_DIRECTORY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f31f65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from run directory (loads best_model.pt by default)\n",
    "train_state = TrainState.from_checkpoint_dir(\n",
    "    run_directory=Path(RUN_DIRECTORY),\n",
    "    rank=0,\n",
    "    world_size=1,\n",
    "    local_rank=0,\n",
    "    epoch=None,  # None = load best model\n",
    ")\n",
    "\n",
    "# Get model and set to eval mode\n",
    "model = train_state.model\n",
    "model.eval()\n",
    "\n",
    "# Get validation loader\n",
    "val_loader = train_state.val_loader\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Dataset: {train_state.config.dataset.name}\")\n",
    "print(f\"Architecture: {train_state.config.arch.model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8740facb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on validation data\n",
    "print(\"Getting predictions on validation set...\")\n",
    "\n",
    "all_images = []\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "all_confidences = []\n",
    "\n",
    "device = train_state.device\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Get model predictions\n",
    "        outputs = model(images)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "        # Get predicted class and confidence\n",
    "        confidences, predictions = torch.max(probs, dim=1)\n",
    "\n",
    "        # Store results\n",
    "        all_images.append(images.cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "        all_predictions.append(predictions.cpu())\n",
    "        all_confidences.append(confidences.cpu())\n",
    "\n",
    "# Concatenate all batches\n",
    "all_images = torch.cat(all_images, dim=0)\n",
    "all_labels = torch.cat(all_labels, dim=0)\n",
    "all_predictions = torch.cat(all_predictions, dim=0)\n",
    "all_confidences = torch.cat(all_confidences, dim=0)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct = (all_predictions == all_labels).sum().item()\n",
    "total = len(all_labels)\n",
    "accuracy = 100.0 * correct / total\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Total samples: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a0f93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most confident correct predictions\n",
    "correct_mask = all_predictions == all_labels\n",
    "correct_indices = torch.where(correct_mask)[0]\n",
    "correct_confidences = all_confidences[correct_indices]\n",
    "\n",
    "# Get top 5 most confident correct predictions\n",
    "top5_correct_indices = correct_indices[\n",
    "    torch.argsort(correct_confidences, descending=True)[:5]\n",
    "]\n",
    "\n",
    "# Find most confident incorrect predictions\n",
    "incorrect_mask = ~correct_mask\n",
    "incorrect_indices = torch.where(incorrect_mask)[0]\n",
    "incorrect_confidences = all_confidences[incorrect_indices]\n",
    "\n",
    "# Get top 5 most confident incorrect predictions\n",
    "top5_incorrect_indices = incorrect_indices[\n",
    "    torch.argsort(incorrect_confidences, descending=True)[:5]\n",
    "]\n",
    "\n",
    "print(f\"Found {len(correct_indices)} correct predictions\")\n",
    "print(f\"Found {len(incorrect_indices)} incorrect predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3a6b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class names directly from the dataset\n",
    "class_names = train_state.val_dataset.classes\n",
    "print(f\"Dataset has {len(class_names)} classes\")\n",
    "\n",
    "\n",
    "def denormalize_image(img, mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616]):\n",
    "    \"\"\"Denormalize image for visualization.\"\"\"\n",
    "    img = img.clone()\n",
    "    for t, m, s in zip(img, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return torch.clamp(img, 0, 1)\n",
    "\n",
    "\n",
    "def plot_predictions(indices, title):\n",
    "    \"\"\"Plot images with their predictions.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "    fig.suptitle(title, fontsize=16, y=1.05)\n",
    "\n",
    "    for idx, ax in enumerate(axes):\n",
    "        img_idx = indices[idx].item()\n",
    "\n",
    "        # Get image and denormalize\n",
    "        img = all_images[img_idx]\n",
    "        img = denormalize_image(img)\n",
    "        img = img.permute(1, 2, 0).numpy()\n",
    "\n",
    "        # Get labels and confidence\n",
    "        true_label = all_labels[img_idx].item()\n",
    "        pred_label = all_predictions[img_idx].item()\n",
    "        confidence = all_confidences[img_idx].item()\n",
    "\n",
    "        # Plot\n",
    "        ax.imshow(img)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        # Title with true and predicted labels\n",
    "        true_name = class_names[true_label]\n",
    "        pred_name = class_names[pred_label]\n",
    "\n",
    "        if true_label == pred_label:\n",
    "            color = \"green\"\n",
    "            title_text = f\"✓ {true_name}\\nConf: {confidence:.2%}\"\n",
    "        else:\n",
    "            color = \"red\"\n",
    "            title_text = (\n",
    "                f\"✗ True: {true_name}\\nPred: {pred_name}\\nConf: {confidence:.2%}\"\n",
    "            )\n",
    "\n",
    "        ax.set_title(title_text, fontsize=10, color=color)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa64498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 5 most confident CORRECT predictions\n",
    "plot_predictions(top5_correct_indices, \"5 Most Confident CORRECT Predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67f4b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 5 most confident INCORRECT predictions\n",
    "plot_predictions(top5_incorrect_indices, \"5 Most Confident INCORRECT Predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6649685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"CONFIDENCE STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nCorrect Predictions:\")\n",
    "print(f\"  Count: {len(correct_indices)}\")\n",
    "print(f\"  Mean Confidence: {correct_confidences.mean():.2%}\")\n",
    "print(f\"  Max Confidence: {correct_confidences.max():.2%}\")\n",
    "print(f\"  Min Confidence: {correct_confidences.min():.2%}\")\n",
    "\n",
    "print(f\"\\nIncorrect Predictions:\")\n",
    "print(f\"  Count: {len(incorrect_indices)}\")\n",
    "print(f\"  Mean Confidence: {incorrect_confidences.mean():.2%}\")\n",
    "print(f\"  Max Confidence: {incorrect_confidences.max():.2%}\")\n",
    "print(f\"  Min Confidence: {incorrect_confidences.min():.2%}\")\n",
    "\n",
    "# Plot confidence distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].hist(\n",
    "    correct_confidences.numpy(), bins=50, alpha=0.7, color=\"green\", edgecolor=\"black\"\n",
    ")\n",
    "axes[0].set_title(\"Confidence Distribution - Correct Predictions\")\n",
    "axes[0].set_xlabel(\"Confidence\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].axvline(\n",
    "    correct_confidences.mean(),\n",
    "    color=\"darkgreen\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Mean: {correct_confidences.mean():.2%}\",\n",
    ")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(\n",
    "    incorrect_confidences.numpy(), bins=50, alpha=0.7, color=\"red\", edgecolor=\"black\"\n",
    ")\n",
    "axes[1].set_title(\"Confidence Distribution - Incorrect Predictions\")\n",
    "axes[1].set_xlabel(\"Confidence\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "axes[1].axvline(\n",
    "    incorrect_confidences.mean(),\n",
    "    color=\"darkred\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Mean: {incorrect_confidences.mean():.2%}\",\n",
    ")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
